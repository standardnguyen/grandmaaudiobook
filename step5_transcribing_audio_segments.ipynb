{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        audio_data = audio_file.read()\n",
    "    \n",
    "    response = openai.Audio.transcribe(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_data,\n",
    "        response_format=\"text\",\n",
    "        language=\"vi\"\n",
    "    )\n",
    "    \n",
    "    transcript = response[\"text\"]\n",
    "    return transcript\n",
    "\n",
    "# Define directories\n",
    "audio_dir = \"dataset/audio_segments\"\n",
    "transcripts_dir = \"dataset/transcripts\"\n",
    "\n",
    "# Ensure the transcripts directory exists\n",
    "os.makedirs(transcripts_dir, exist_ok=True)\n",
    "\n",
    "# Transcribe each audio segment\n",
    "transcripts = []\n",
    "for audio_file in os.listdir(audio_dir):\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(audio_dir, audio_file)\n",
    "        transcript = transcribe_audio(audio_path)\n",
    "        transcripts.append((audio_file, transcript))\n",
    "        transcript_path = os.path.join(transcripts_dir, f\"transcript_{audio_file.replace('.wav', '.txt')}\")\n",
    "        with open(transcript_path, \"w\") as f:\n",
    "            f.write(transcript)\n",
    "\n",
    "print(f\"Transcripts saved in {transcripts_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
